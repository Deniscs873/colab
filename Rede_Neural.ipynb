{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deniscs873/colab/blob/main/Rede_Neural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Redes neurais\n",
        "\n",
        "Rede Neural Artificial (RNA) pode ser definida como uma estrutura complexa interligada por elementos de processamento simples (neur√¥nios), que possuem a capacidade de realizar opera√ß√µes como c√°lculos em paralelo, para processamento de dados e representa√ß√£o de conhecimento. Seu primeiro conceito foi introduzido em 1943, mas ganhou popularidade algumas d√©cadas depois com a introdu√ß√£o de algoritmos de treinamento como o backpropagation, que permite a realiza√ß√£o de um treinamento posterior para aperfei√ßoar os resultados do modelo."
      ],
      "metadata": {
        "id": "mPVvnxTTpd3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estudo: Conjunto de dados de sementes de trigo üìö\n",
        "\n"
      ],
      "metadata": {
        "id": "nbLkuizApZaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O conjunto de dados de sementes envolve a previs√£o de esp√©cies com medidas de sementes de diferentes variedades de trigo.\n",
        "\n",
        "Existem 201 registros e 7 vari√°veis ‚Äã‚Äãde entrada num√©ricas. √â um problema de classifica√ß√£o com 3 classes de sa√≠da. A escala para cada valor num√©rico de entrada varia, portanto, pode ser necess√°ria alguma normaliza√ß√£o de dados para uso com algoritmos que ponderam entradas como o algoritmo de retropropaga√ß√£o.\n",
        "\n",
        "Voc√™ pode aprender mais e baixar o conjunto de dados de sementes no [Reposit√≥rio de Aprendizado de M√°quina da UCI](http://archive.ics.uci.edu/ml/datasets/seeds).\n",
        "- Fa√ßa o download do conjunto de dados de sementes e coloque-o em seu diret√≥rio de trabalho atual com o nome de arquivo **seeds_dataset.csv**.\n",
        "- O conjunto de dados est√° no formato separado por tabula√ß√£o, portanto, voc√™ deve convert√™-lo para CSV usando um editor de texto ou um programa de planilha. Se preferir, pode fazer download do conjunto de dados no formato CSV diretamente:\n",
        "  - [Baixar conjunto de dados de sementes de trigo](https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv)"
      ],
      "metadata": {
        "id": "_PrKGBz1p4fD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importando as bibliotecas"
      ],
      "metadata": {
        "id": "anMhWtoCq0hU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Backprop on the Seeds Dataset\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from random import random\n",
        "from csv import reader\n",
        "from math import exp"
      ],
      "metadata": {
        "id": "xlyPx3SQKNn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inicialize a rede"
      ],
      "metadata": {
        "id": "tl5KvENGq2fE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos come√ßar com algo f√°cil, a cria√ß√£o de uma nova rede pronta para treinamento.\n",
        "\n",
        "Cada neur√¥nio tem um conjunto de pesos que precisam ser mantidos. Um peso para cada conex√£o de entrada e um peso adicional para a polariza√ß√£o. Precisamos armazenar propriedades adicionais para um neur√¥nio durante o treinamento; portanto, usaremos um dicion√°rio para representar cada neur√¥nio e armazenar propriedades por nomes como `pesos (weights)` para os pesos.\n",
        "\n",
        "Uma rede √© organizada em camadas. A camada de entrada √© realmente apenas uma linha do nosso conjunto de dados de treinamento. A primeira camada real √© a camada oculta. Isso √© seguido pela camada de sa√≠da que possui um neur√¥nio para cada valor de classe.\n",
        "\n",
        "Organizaremos as camadas como matrizes de dicion√°rios e trataremos toda a rede como uma matriz de camadas.\n",
        "\n",
        "√â uma boa pr√°tica inicializar os pesos da rede para pequenos n√∫meros aleat√≥rios. Nesse caso, usaremos n√∫meros aleat√≥rios no intervalo de 0 a 1.\n",
        "\n",
        "Abaixo est√° uma fun√ß√£o denominada `initialize_network()` que cria uma nova rede neural pronta para treinamento. Ele aceita tr√™s par√¢metros, o n√∫mero de entradas, o n√∫mero de neur√¥nios a ter na camada oculta e o n√∫mero de sa√≠das.\n",
        "\n",
        "Voc√™ pode ver que, para a camada oculta, criamos `n_hidden` neur√¥nios e cada neur√¥nio na camada oculta possui `n_inputs + 1` pesos, um para cada coluna de entrada em um conjunto de dados e um adicional para o vi√©s.\n",
        "\n",
        "Voc√™ tamb√©m pode ver que a camada de sa√≠da que se conecta √† camada oculta possui `n_outputs` de neur√¥nios, cada um com `n_hidden + 1` pesos. Isso significa que cada neur√¥nio na camada de sa√≠da se conecta a (tem um peso para) cada neur√¥nio na camada oculta."
      ],
      "metadata": {
        "id": "LtcZb2bRtOPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a network\n",
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "   network = list()\n",
        "   hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
        "   network.append(hidden_layer)\n",
        "   output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
        "   network.append(output_layer)\n",
        "   return network"
      ],
      "metadata": {
        "id": "-woxLb7nsGgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos testar esta fun√ß√£o. Abaixo est√° um exemplo completo que cria uma pequena rede."
      ],
      "metadata": {
        "id": "YH3LDRTbspd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import seed\n",
        "from random import random\n",
        "# Initialize a network\n",
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "   network = list()\n",
        "   hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
        "   network.append(hidden_layer)\n",
        "   output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
        "   network.append(output_layer)\n",
        "   return network\n",
        "\n",
        "seed(1)\n",
        "network = initialize_network(2, 1, 2)\n",
        "for layer in network:\n",
        "   print(layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veYjxzgJsp3g",
        "outputId": "20853332-d67d-4889-9de3-9f9c00ecb243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]\n",
            "[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executando o exemplo, voc√™ pode ver que o c√≥digo imprime cada camada uma por uma. Voc√™ pode ver que a camada oculta possui um neur√¥nio com 2 pesos de entrada mais o vi√©s. A camada de sa√≠da possui 2 neur√¥nios, cada um com 1 peso mais o vi√©s."
      ],
      "metadata": {
        "id": "COZcYJkus0Ph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ñ∂ Agora que sabemos como criar e inicializar uma rede, vamos ver como podemos us√°-la para calcular uma sa√≠da."
      ],
      "metadata": {
        "id": "LUsuC8eus8mj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Propagar para a frente (Forward Propagate)"
      ],
      "metadata": {
        "id": "ge9U69uatE1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos calcular uma sa√≠da de uma rede neural propagando um sinal de entrada atrav√©s de cada camada at√© que a camada de sa√≠da produza seus valores.\n",
        "\n",
        "Chamamos isso de **propaga√ß√£o direta**.\n",
        "\n",
        "√â a t√©cnica de que precisamos para gerar previs√µes durante o treinamento que precisar√° ser corrigida e √© o m√©todo que precisaremos depois que a rede for treinada para fazer previs√µes de novos dados.\n",
        "\n",
        "Podemos dividir a propaga√ß√£o em tr√™s partes:\n",
        "1.  Ativa√ß√£o do neur√¥nio.\n",
        "2.  Transfer√™ncia de neur√¥nios\n",
        "3.  Propaga√ß√£o para a frente."
      ],
      "metadata": {
        "id": "mWGXUG_gtVyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ativa√ß√£o do Neur√¥nio"
      ],
      "metadata": {
        "id": "wxtgTv-Kt1z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O primeiro passo √© calcular a ativa√ß√£o de um neur√¥nio que recebe uma entrada.\n",
        "\n",
        "A entrada pode ser uma linha do nosso conjunto de dados de treinamento, como no caso da camada oculta. Tamb√©m podem ser as sa√≠das de cada neur√¥nio na camada oculta, no caso da camada de sa√≠da.\n",
        "\n",
        "A ativa√ß√£o do neur√¥nio √© calculada como a soma ponderada das entradas. Muito parecido com regress√£o linear.\n",
        "\n",
        "`activation = sum(weight_i * input_i) + bias`\n",
        "\n",
        "Onde `peso` √© um peso de rede, `entrada` √© uma entrada, `i` √© o √≠ndice de um peso ou uma entrada e `vi√©s` √© um peso especial que n√£o tem entrada para multiplicar (ou voc√™ pode pensar na entrada como sempre sendo 1.0).\n",
        "\n",
        "Abaixo est√° uma implementa√ß√£o disso em uma fun√ß√£o chamada `activate()`. Voc√™ pode ver que a fun√ß√£o assume que o vi√©s √© o √∫ltimo peso na lista de pesos. Isso ajuda aqui e mais tarde a facilitar a leitura do c√≥digo."
      ],
      "metadata": {
        "id": "BA3Ljm1-t5k2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate neuron activation for an input\n",
        "def activate(weights, inputs):\n",
        "   activation = weights[-1]\n",
        "   for i in range(len(weights)-1):\n",
        "      activation += weights[i] * inputs[i]\n",
        "   return activation"
      ],
      "metadata": {
        "id": "JcySKe9muBI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, vamos ver como usar a ativa√ß√£o do neur√¥nio."
      ],
      "metadata": {
        "id": "2gjiSpiiubT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transfer√™ncia de neur√¥nios"
      ],
      "metadata": {
        "id": "ieP55uWXuhZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depois que um neur√¥nio √© ativado, precisamos transferir a ativa√ß√£o para ver qual √© realmente a sa√≠da do neur√¥nio.\n",
        "\n",
        "Diferentes fun√ß√µes de transfer√™ncia podem ser usadas. √â tradicional usar a [**fun√ß√£o de ativa√ß√£o sigm√≥ide**](https://en.wikipedia.org/wiki/Sigmoid_function) , mas voc√™ tamb√©m pode usar a [**fun√ß√£o tanh ( tangente hiperb√≥lica )**](https://en.wikipedia.org/wiki/Hyperbolic_functions) para transferir sa√≠das. Mais recentemente, a [**fun√ß√£o de transfer√™ncia de retificadores**](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) tem sido popular em grandes redes de aprendizado profundo.\n",
        "\n",
        "A fun√ß√£o de ativa√ß√£o sigm√≥ide se parece com uma forma de S, tamb√©m √© chamada de fun√ß√£o log√≠stica. Ele pode pegar qualquer valor de entrada e produzir um n√∫mero entre 0 e 1 em uma curva S. Tamb√©m √© uma fun√ß√£o da qual podemos calcular facilmente a derivada (inclina√ß√£o) de que precisaremos mais tarde ao retropropagar o erro.\n",
        "\n",
        "Podemos transferir uma fun√ß√£o de ativa√ß√£o usando a fun√ß√£o sigm√≥ide da seguinte maneira:\n",
        "`output = 1 / (1 + e^(-activation))`\n",
        "\n",
        "Onde `e` √© a base dos logaritmos naturais ([n√∫mero de Euler](https://en.wikipedia.org/wiki/E_(mathematical_constant))).\n",
        "\n",
        "Abaixo est√° uma fun√ß√£o chamada `transfer()` que implementa a equa√ß√£o sigm√≥ide."
      ],
      "metadata": {
        "id": "nK87AMW1ui_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfer neuron activation\n",
        "def transfer(activation):\n",
        "   return 1.0 / (1.0 + exp(-activation))"
      ],
      "metadata": {
        "id": "_0bD1l_ouaAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora que temos as pe√ßas, vamos ver como elas s√£o usadas."
      ],
      "metadata": {
        "id": "VmLoH5HbvXge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Propaga√ß√£o direta"
      ],
      "metadata": {
        "id": "EKaoQ8snvZeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A propaga√ß√£o direta de uma entrada √© simples.\n",
        "\n",
        "Trabalhamos atrav√©s de cada camada da nossa rede, calculando as sa√≠das para cada neur√¥nio. Todas as sa√≠das de uma camada se tornam entradas para os neur√¥nios na pr√≥xima camada.\n",
        "\n",
        "Abaixo est√° uma fun√ß√£o chamada `forward_propagate()` que implementa a propaga√ß√£o direta de uma linha de dados de nosso conjunto de dados com nossa rede neural.\n",
        "\n",
        "Voc√™ pode ver que o valor de sa√≠da de um neur√¥nio √© armazenado no neur√¥nio com o nome `output`. Voc√™ tamb√©m pode ver que coletamos as sa√≠das para uma camada em uma matriz chamada new_inputs que se torna a `entrada (inputs)` da matriz e √© usada como entrada para a camada a seguir.\n",
        "\n",
        "A fun√ß√£o retorna as sa√≠das da √∫ltima camada, tamb√©m chamada camada de sa√≠da."
      ],
      "metadata": {
        "id": "LcZDd4MvvcWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward propagate input to a network output\n",
        "def forward_propagate(network, row):\n",
        "   inputs = row\n",
        "   for layer in network:\n",
        "      new_inputs = []\n",
        "      for neuron in layer:\n",
        "         activation = activate(neuron['weights'], inputs)\n",
        "         neuron['output'] = transfer(activation)\n",
        "         new_inputs.append(neuron['output'])\n",
        "      inputs = new_inputs\n",
        "   return inputs"
      ],
      "metadata": {
        "id": "axq43R9tvrs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Exemplo"
      ],
      "metadata": {
        "id": "THgckyicvqnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos juntar todas essas pe√ßas e testar a propaga√ß√£o direta da nossa rede.\n",
        "\n",
        "Definimos nossa rede em linha com um neur√¥nio oculto que espera 2 valores de entrada e uma camada de sa√≠da com dois neur√¥nios."
      ],
      "metadata": {
        "id": "fhsTsL6HwB3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test forward propagation\n",
        "network = [[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}], [{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
        "row = [1, 0, None]\n",
        "output = forward_propagate(network, row)\n",
        "print(output)\n",
        "\n",
        "# A execu√ß√£o do exemplo propaga o padr√£o de entrada [1, 0] e produz um valor de sa√≠da que √© impresso. Como a camada de sa√≠da possui dois neur√¥nios, obtemos uma lista de dois n√∫meros como sa√≠da.\n",
        "# Os valores reais de sa√≠da s√£o apenas absurdos por enquanto, mas a seguir, come√ßaremos a aprender como tornar os pesos nos neur√¥nios mais √∫teis."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlsqWIY3wK1K",
        "outputId": "d9c94ebd-cd3d-41a0-b838-6b584afef0b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6629970129852887, 0.7253160725279748]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Erro de Propaga√ß√£o Traseira (Back Propagate Error)"
      ],
      "metadata": {
        "id": "vLNbj_L_w1as"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O algoritmo de retropropaga√ß√£o √© nomeado para a maneira como os pesos s√£o treinados.\n",
        "\n",
        "O erro √© calculado entre as sa√≠das esperadas e as sa√≠das propagadas a partir da rede. Esses erros s√£o propagados para tr√°s pela rede, da camada de sa√≠da para a camada oculta, atribuindo a culpa pelo erro e atualizando os pesos √† medida que avan√ßam.\n",
        "\n",
        "A matem√°tica para o erro de retropropaga√ß√£o est√° enraizada no c√°lculo, mas permaneceremos em alto n√≠vel nesta se√ß√£o e focaremos no que √© calculado e como, e n√£o por que, os c√°lculos assumem esse formato espec√≠fico.\n",
        "\n",
        "Esta parte √© dividida em duas se√ß√µes.\n",
        "1. Derivado de transfer√™ncia.\n",
        "2. Backpropagation de erro."
      ],
      "metadata": {
        "id": "wvFQyYyyw7DP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Derivado de transfer√™ncia"
      ],
      "metadata": {
        "id": "aic0Gh7xxA8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado um valor de sa√≠da de um neur√¥nio, precisamos calcular sua inclina√ß√£o.\n",
        "\n",
        "Estamos usando a fun√ß√£o de transfer√™ncia sigm√≥ide, cuja derivada pode ser calculada da seguinte maneira:\n",
        "`derivative = output * (1.0 - output)`\n",
        "\n",
        "Abaixo est√° uma fun√ß√£o chamada `transfer_derivative()` que implementa esta equa√ß√£o."
      ],
      "metadata": {
        "id": "FTsaKOwGxEFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the derivative of an neuron output\n",
        "def transfer_derivative(output):\n",
        "   return output * (1.0 - output)"
      ],
      "metadata": {
        "id": "yiVSfZZ1xPFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, vamos ver como isso pode ser usado."
      ],
      "metadata": {
        "id": "cib_yncoxRf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Backpropagation de erro (Error Backpropagation)"
      ],
      "metadata": {
        "id": "3I6lqbQxxUC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O primeiro passo √© calcular o erro para cada neur√¥nio de sa√≠da, isso nos dar√° nosso sinal de erro (entrada) para propagar para tr√°s atrav√©s da rede.\n",
        "\n",
        "O erro para um determinado neur√¥nio pode ser calculado da seguinte maneira:\n",
        "`error = (expected - output) * transfer_derivative(output)`\n",
        "\n",
        "Onde `expected` √© o valor de sa√≠da esperado para o neur√¥nio, `output` √© o valor de sa√≠da para o neur√¥nio e `transfer_derivative()` calcula a inclina√ß√£o do valor de sa√≠da do neur√¥nio, como mostrado acima.\n",
        "\n",
        "Este c√°lculo de erro √© usado para neur√¥nios na camada de sa√≠da (output layer). O valor esperado √© o pr√≥prio valor da classe. Na camada oculta, as coisas s√£o um pouco mais complicadas.\n",
        "\n",
        "O sinal de erro de um neur√¥nio na camada oculta √© calculado como o erro ponderado de cada neur√¥nio na camada de sa√≠da. Pense no erro retornando ao longo dos pesos da camada de sa√≠da at√© os neur√¥nios na camada oculta.\n",
        "\n",
        "O sinal de erro propagado de volta √© acumulado e, em seguida, usado para determinar o erro do neur√¥nio na camada oculta, da seguinte maneira:\n",
        "\n",
        "`error = (weight_k * error_j) * transfer_derivative(output)`\n",
        "\n",
        "Onde `error_j` √© o sinal de erro do `j-` √©simo neur√¥nio na camada de sa√≠da, `weight_k` √© o peso que conecta o `k-` √©simo neur√¥nio ao neur√¥nio atual e a sa√≠da √© a sa√≠da do neur√¥nio atual.\n",
        "\n",
        "Abaixo est√° uma fun√ß√£o chamada `backward_propagate_error()` que implementa este procedimento.\n",
        "\n",
        "Voc√™ pode ver que o sinal de erro calculado para cada neur√¥nio √© armazenado com o nome ‚Äòdelta‚Äô. Voc√™ pode ver que as camadas da rede s√£o iteradas na ordem inversa, come√ßando na sa√≠da e trabalhando para tr√°s. Isso garante que os neur√¥nios na camada de sa√≠da tenham valores ‚Äòdelta‚Äô calculados primeiro que os neur√¥nios na camada oculta possam usar na itera√ß√£o subsequente. Eu escolhi o nome ‚Äòdelta‚Äô para refletir a altera√ß√£o que o erro implica no neur√¥nio (por exemplo, o delta do peso).\n",
        "\n",
        "Voc√™ pode ver que o sinal de erro para neur√¥nios na camada oculta √© acumulado a partir de neur√¥nios na camada de sa√≠da, onde o n√∫mero de neur√¥nios ocultos `j` tamb√©m √© o √≠ndice do peso do neur√¥nio no neur√¥nio da camada de sa√≠da `neuron[‚Äòweights‚Äô][j]`."
      ],
      "metadata": {
        "id": "b9aRh099xXDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagate error and store in neurons\n",
        "def backward_propagate_error(network, expected):\n",
        "   for i in reversed(range(len(network))):\n",
        "      layer = network[i]\n",
        "      errors = list()\n",
        "      if i != len(network)-1:\n",
        "         for j in range(len(layer)):\n",
        "            error = 0.0\n",
        "            for neuron in network[i + 1]:\n",
        "               error += (neuron['weights'][j] * neuron['delta'])\n",
        "            errors.append(error)\n",
        "      else:\n",
        "         for j in range(len(layer)):\n",
        "            neuron = layer[j]\n",
        "            errors.append(expected[j] - neuron['output'])\n",
        "      for j in range(len(layer)):\n",
        "         neuron = layer[j]\n",
        "         neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])"
      ],
      "metadata": {
        "id": "ejiTVx0px_JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exemplo - vamos ver funcionando at√© esse trecho"
      ],
      "metadata": {
        "id": "dP9peu8WyFQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test backpropagation of error\n",
        "network = [[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
        "[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095]}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
        "expected = [0, 1]\n",
        "backward_propagate_error(network, expected)\n",
        "for layer in network:\n",
        "   print(layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D_Fv9OQyJLW",
        "outputId": "e079b6e1-1c0a-4637-f6aa-7733f72b2a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614], 'delta': -0.0005348048046610517}]\n",
            "[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095], 'delta': -0.14619064683582808}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763], 'delta': 0.0771723774346327}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A execu√ß√£o do exemplo imprime a rede ap√≥s a conclus√£o da backpropagation de erro. Voc√™ pode ver que os valores de erro s√£o calculados e armazenados nos neur√¥nios da camada de sa√≠da e da camada oculta."
      ],
      "metadata": {
        "id": "LGKQ9YGIyNnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos usar a backpropagation de erro para treinar a rede."
      ],
      "metadata": {
        "id": "Uqm_WZlqyQNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rede de treinamento (Train Network)"
      ],
      "metadata": {
        "id": "IHuVOcd3yR3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A rede √© treinada usando descida de gradiente estoc√°stico.\n",
        "\n",
        "Isso envolve v√°rias itera√ß√µes de exposi√ß√£o de um conjunto de dados de treinamento √† rede e, para cada linha de dados, propagando as entradas, *backpropagating* o erro e atualizando os pesos da rede.\n",
        "\n",
        "Esta parte √© dividida em duas se√ß√µes:\n",
        "1. Atualizar pesos.\n",
        "2. Rede de treinamento."
      ],
      "metadata": {
        "id": "1k_d_Hm6yV6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Atualizar pesos (Update Weights)"
      ],
      "metadata": {
        "id": "BiPTS0AHyc4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma vez que os erros s√£o calculados para cada neur√¥nio na rede pelo m√©todo de propaga√ß√£o de retorno acima, eles podem ser usados ‚Äã‚Äãpara atualizar pesos.\n",
        "\n",
        "Os pesos da rede s√£o atualizados da seguinte maneira:\n",
        "\n",
        "`weight = weight + learning_rate * error * input`\n",
        "\n",
        "Onde `weight` √© um dado peso, `learning_rate` √© um par√¢metro que voc√™ deve especificar, `error` √© o erro calculado pelo procedimento de retropropaga√ß√£o para o neur√¥nio e `input` √© o valor de entrada que causou o erro.\n",
        "\n",
        "O mesmo procedimento pode ser usado para atualizar o peso da polariza√ß√£o, exceto que n√£o h√° termo de entrada ou a entrada √© o valor fixo de 1.0.\n",
        "\n",
        "A taxa de aprendizado controla quanto alterar o peso para corrigir o erro. Por exemplo, um valor de 0.1 atualizar√° o peso em 10% da quantia que possivelmente poderia ser atualizada. Preferem-se pequenas taxas de aprendizado que causam aprendizado mais lento em um grande n√∫mero de itera√ß√µes de treinamento. Isso aumenta a probabilidade de a rede encontrar um bom conjunto de pesos em todas as camadas, em vez do conjunto mais r√°pido de pesos que minimiza o erro (chamado converg√™ncia prematura).\n",
        "\n",
        "Abaixo est√° uma fun√ß√£o chamada `update_weights()` que atualiza os pesos para uma rede, dada uma linha de dados de entrada, uma taxa de aprendizado e assume que uma propaga√ß√£o para frente e para tr√°s j√° foi executada.\n",
        "\n",
        "Lembre-se de que a entrada para a camada de sa√≠da √© uma cole√ß√£o de sa√≠das da camada oculta."
      ],
      "metadata": {
        "id": "WTkb02uhygRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update network weights with error\n",
        "def update_weights(network, row, l_rate):\n",
        "   for i in range(len(network)):\n",
        "      inputs = row[:-1]\n",
        "      if i != 0:\n",
        "         inputs = [neuron['output'] for neuron in network[i - 1]]\n",
        "      for neuron in network[i]:\n",
        "         for j in range(len(inputs)):\n",
        "            neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
        "         neuron['weights'][-1] += l_rate * neuron['delta']"
      ],
      "metadata": {
        "id": "cHcLKQ6Ky39E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora que sabemos como atualizar os pesos da rede, vamos ver como podemos fazer isso repetidamente."
      ],
      "metadata": {
        "id": "k962vYujy6q0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rede de Treinamento (Train Network)"
      ],
      "metadata": {
        "id": "PDKQ_zy_y8jU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como mencionado, a rede √© atualizada usando a descida estoc√°stica do gradiente.\n",
        "\n",
        "Isso envolve o primeiro loop para um n√∫mero fixo de √©pocas e, em cada √©poca, a atualiza√ß√£o da rede para cada linha no conjunto de dados de treinamento.\n",
        "\n",
        "Como s√£o feitas atualiza√ß√µes para cada padr√£o de treinamento, esse tipo de aprendizado √© chamado de aprendizado on-line. Se erros foram acumulados em uma √©poca antes de atualizar os pesos, isso √© chamado aprendizado em lote ou descida em gradiente em lote.\n",
        "\n",
        "Abaixo est√° uma fun√ß√£o que implementa o treinamento de uma rede neural j√° inicializada com um determinado conjunto de dados de treinamento, taxa de aprendizado, n√∫mero fixo de √©pocas e n√∫mero esperado de valores de sa√≠da.\n",
        "\n",
        "O n√∫mero esperado de valores de sa√≠da √© usado para transformar valores de classe nos dados de treinamento em uma codifica√ß√£o quente. Esse √© um vetor bin√°rio com uma coluna para cada valor de classe para corresponder √† sa√≠da da rede. Isso √© necess√°rio para calcular o erro para a camada de sa√≠da.\n",
        "\n",
        "Voc√™ tamb√©m pode ver que o erro da soma ao quadrado entre a sa√≠da esperada e a sa√≠da da rede √© acumulado a cada √©poca e impresso. Isso √© √∫til para criar um rastro do quanto a rede est√° aprendendo e melhorando a cada √©poca."
      ],
      "metadata": {
        "id": "azNdmUIRy-6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a network for a fixed number of epochs\n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "   for epoch in range(n_epoch):\n",
        "      for row in train:\n",
        "         outputs = forward_propagate(network, row)\n",
        "         expected = [0 for i in range(n_outputs)]\n",
        "         expected[row[-1]] = 1\n",
        "         backward_propagate_error(network, expected)\n",
        "         update_weights(network, row, l_rate)"
      ],
      "metadata": {
        "id": "eqbIoQrgy6WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora temos todas as pe√ßas para treinar a rede. Podemos montar um exemplo que inclui tudo o que vimos at√© agora, incluindo inicializa√ß√£o de rede e treinar uma rede em um pequeno conjunto de dados.\n",
        "\n",
        "Abaixo est√° um pequeno conjunto de dados artificial que podemos usar para testar o treinamento de nossa **rede neural**.\n",
        "\n",
        "```\n",
        "X1          X2           Y\n",
        "2.7810836   2.550537003  0\n",
        "1.465489372 2.362125076  0\n",
        "3.396561688 4.400293529  0\n",
        "1.38807019  1.850220317  0\n",
        "3.06407232  3.005305973  0\n",
        "7.627531214 2.759262235  1\n",
        "5.332441248 2.088626775  1\n",
        "6.922596716 1.77106367   1\n",
        "8.675418651 -0.242068655 1\n",
        "7.673756466 3.508563011  1\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "TdBoTSjszGkn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Testando\n",
        "\n",
        "Abaixo est√° o exemplo completo. Vamos usar 2 neur√¥nios na camada oculta. √â um problema de classifica√ß√£o bin√°ria (2 classes), portanto haver√° dois neur√¥nios na camada de sa√≠da. A rede ser√° treinada por 20 √©pocas com uma taxa de aprendizado de 0,5, o que √© alto porque estamos treinando para poucas itera√ß√µes."
      ],
      "metadata": {
        "id": "dM-Alm0KzT5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test training backprop algorithm\n",
        "seed(1)\n",
        "dataset = [[2.7810836,2.550537003,0],\n",
        "           [1.465489372,2.362125076,0],\n",
        "           [3.396561688,4.400293529,0],\n",
        "           [1.38807019,1.850220317,0],\n",
        "           [3.06407232,3.005305973,0],\n",
        "           [7.627531214,2.759262235,1],\n",
        "           [5.332441248,2.088626775,1],\n",
        "           [6.922596716,1.77106367,1],\n",
        "           [8.675418651,-0.242068655,1],\n",
        "           [7.673756466,3.508563011,1]]\n",
        "\n",
        "n_inputs = len(dataset[0]) - 1\n",
        "n_outputs = len(set([row[-1] for row in dataset]))\n",
        "network = initialize_network(n_inputs, 2, n_outputs)\n",
        "\n",
        "train_network(network, dataset, 0.5, 20, n_outputs)\n",
        "\n",
        "for layer in network:\n",
        "   print(layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYRWGg0xzeI8",
        "outputId": "a7635338-1c29-451e-8ee7-d04170cca21b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'weights': [-1.4688375095432327, 1.850887325439514, 1.0858178629550297], 'output': 0.029980305604426185, 'delta': -0.0059546604162323625}, {'weights': [0.37711098142462157, -0.0625909894552989, 0.2765123702642716], 'output': 0.9456229000211323, 'delta': 0.0026279652850863837}]\n",
            "[{'weights': [2.515394649397849, -0.3391927502445985, -0.9671565426390275], 'output': 0.23648794202357587, 'delta': -0.04270059278364587}, {'weights': [-2.5584149848484263, 1.0036422106209202, 0.42383086467582715], 'output': 0.7790535202438367, 'delta': 0.03803132596437354}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import exp\n",
        "from random import seed\n",
        "from random import random\n",
        "\n",
        "# Train a network for a fixed number of epochs\n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "   for epoch in range(n_epoch):\n",
        "      sum_error = 0\n",
        "   for row in train:\n",
        "      outputs = forward_propagate(network, row)\n",
        "      expected = [0 for i in range(n_outputs)]\n",
        "      expected[row[-1]] = 1\n",
        "      sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
        "      backward_propagate_error(network, expected)\n",
        "      update_weights(network, row, l_rate)\n",
        "   print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
        "\n",
        "# Test training backprop algorithm\n",
        "seed(1)\n",
        "dataset = [[2.7810836,2.550537003,0],\n",
        "           [1.465489372,2.362125076,0],\n",
        "           [3.396561688,4.400293529,0],\n",
        "           [1.38807019,1.850220317,0],\n",
        "           [3.06407232,3.005305973,0],\n",
        "           [7.627531214,2.759262235,1],\n",
        "           [5.332441248,2.088626775,1],\n",
        "           [6.922596716,1.77106367,1],\n",
        "           [8.675418651,-0.242068655,1],\n",
        "           [7.673756466,3.508563011,1]]\n",
        "n_inputs = len(dataset[0]) - 1\n",
        "n_outputs = len(set([row[-1] for row in dataset]))\n",
        "network = initialize_network(n_inputs, 2, n_outputs)\n",
        "train_network(network, dataset, 0.5, 20, n_outputs)\n",
        "for layer in network:\n",
        "   print(layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeCs3KE2z8DY",
        "outputId": "b81ea871-b3ea-4308-b301-4928ec1b7800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">epoch=19, lrate=0.500, error=6.350\n",
            "[{'weights': [0.04702244528095891, 0.8442898057367274, 0.7548289994028103], 'output': 0.9840590187415943, 'delta': -0.0012432966701953782}, {'weights': [0.1722759810012958, 0.4447229928742587, 0.42635133323053526], 'output': 0.9663709282875607, 'delta': -0.0013702294823488327}]\n",
            "[{'weights': [0.40923052462427756, 0.54380310303148, -0.16580838069853054], 'output': 0.7255320776037567, 'delta': -0.1444790348531614}, {'weights': [-0.11119113899945762, 0.7152027899877591, 0.2916019058174817], 'output': 0.684483563116403, 'delta': 0.06814076441849203}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A execu√ß√£o do exemplo primeiro imprime o erro da soma ao quadrado a cada √©poca de treinamento. Podemos ver uma tend√™ncia desse erro diminuindo a cada √©poca.\n",
        "\n",
        "Depois de treinada, a rede √© impressa, mostrando os pesos aprendidos. Tamb√©m na rede ainda est√£o os valores de sa√≠da e delta que podem ser ignorados. Poder√≠amos atualizar nossa fun√ß√£o de treinamento para excluir esses dados, se quis√©ssemos."
      ],
      "metadata": {
        "id": "lRpDPrixznaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depois que uma rede √© treinada, precisamos us√°-la para fazer previs√µes. üîÆ"
      ],
      "metadata": {
        "id": "SRKvMgiK0P7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prever (Predict) üîÆ"
      ],
      "metadata": {
        "id": "iTyHkhqe0Xea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fazer previs√µes com uma rede neural treinada √© bastante f√°cil.\n",
        "\n",
        "J√° vimos como propagar adiante um padr√£o de entrada para obter uma sa√≠da. √â tudo o que precisamos fazer para fazer uma previs√£o. Podemos usar os pr√≥prios valores de sa√≠da diretamente como a probabilidade de um padr√£o pertencente a cada classe de sa√≠da.\n",
        "\n",
        "Pode ser mais √∫til transformar essa sa√≠da novamente em uma previs√£o de classe n√≠tida. Podemos fazer isso selecionando o valor da classe com maior probabilidade. Isso tamb√©m √© chamado de [fun√ß√£o arg max](https://en.wikipedia.org/wiki/Arg_max).\n",
        "\n",
        "Abaixo est√° uma fun√ß√£o denominada *predict()* que implementa este procedimento. Retorna o √≠ndice na sa√≠da da rede que tem a maior probabilidade. Parte do princ√≠pio que valores de classe foram convertidos em n√∫meros inteiros come√ßando em 0."
      ],
      "metadata": {
        "id": "2I-4uyzH0fuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction with a network\n",
        "def predict(network, row):\n",
        "   outputs = forward_propagate(network, row)\n",
        "   return outputs.index(max(outputs))"
      ],
      "metadata": {
        "id": "pagwXXdE3CbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exemplo\n",
        "Podemos juntar isso com o c√≥digo acima para entrada de propaga√ß√£o direta e com nosso pequeno conjunto de dados artificial para testar as previs√µes com uma rede j√° treinada. O exemplo codifica uma rede treinada da etapa anterior.\n",
        "\n",
        "O exemplo completo est√° listado abaixo."
      ],
      "metadata": {
        "id": "bv1qe8Ly3GFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test making predictions with the network\n",
        "dataset = [[2.7810836,2.550537003,0],\n",
        "           [1.465489372,2.362125076,0],\n",
        "           [3.396561688,4.400293529,0],\n",
        "           [1.38807019,1.850220317,0],\n",
        "           [3.06407232,3.005305973,0],\n",
        "           [7.627531214,2.759262235,1],\n",
        "           [5.332441248,2.088626775,1],\n",
        "           [6.922596716,1.77106367,1],\n",
        "           [8.675418651,-0.242068655,1],\n",
        "           [7.673756466,3.508563011,1]]\n",
        "network = [[{'weights': [-1.482313569067226, 1.8308790073202204, 1.078381922048799]}, {'weights': [0.23244990332399884, 0.3621998343835864, 0.40289821191094327]}],\n",
        "[{'weights': [2.5001872433501404, 0.7887233511355132, -1.1026649757805829]}, {'weights': [-2.429350576245497, 0.8357651039198697, 1.0699217181280656]}]]\n",
        "\n",
        "for row in dataset:\n",
        "   prediction = predict(network, row)\n",
        "   print('Expected=%d, Got=%d' % (row[-1], prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U2W8Iyh3LfT",
        "outputId": "90cb10d1-7e48-423e-a621-77e2c1ebd484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A execu√ß√£o do exemplo imprime a sa√≠da esperada para cada registro no conjunto de dados de treinamento, seguida pela previs√£o precisa feita pela rede.\n",
        "\n",
        "Isso mostra que a rede atinge 100% de acur√°cia (accuracy) nesse pequeno conjunto de dados."
      ],
      "metadata": {
        "id": "lxUAf8p55oUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëΩ Agora estamos prontos para aplicar nosso algoritmo de *backpropagation* a um conjunto de dados do mundo real."
      ],
      "metadata": {
        "id": "rJGXD5_N3ZVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conjunto de Dados de Sementes de Trigo"
      ],
      "metadata": {
        "id": "uHOf2yIq3biJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta se√ß√£o aplica o algoritmo *Backpropagation* ao conjunto de dados de sementes de trigo.\n",
        "\n",
        "O primeiro passo √© carregar o conjunto de dados e converter os dados carregados em n√∫meros que podemos usar em nossa rede neural. Para isso, usaremos a fun√ß√£o auxiliar `load_csv()` para carregar o arquivo, `str_column_to_float()` para converter n√∫meros de string em flutuantes e `str_column_to_int()` para converter a coluna da classe em valores inteiros."
      ],
      "metadata": {
        "id": "WpqMX6VL3gCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "   dataset = list()\n",
        "   with open(filename, 'r') as file:\n",
        "      csv_reader = reader(file)\n",
        "      for row in csv_reader:\n",
        "         if not row:\n",
        "            continue\n",
        "         dataset.append(row)\n",
        "   return dataset"
      ],
      "metadata": {
        "id": "j6x9hZP-32OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "   for row in dataset:\n",
        "      row[column] = float(row[column].strip())"
      ],
      "metadata": {
        "id": "k2dq2jn5KUZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert string column to integer\n",
        "def str_column_to_int(dataset, column):\n",
        "   class_values = [row[column] for row in dataset]\n",
        "   unique = set(class_values)\n",
        "   lookup = dict()\n",
        "   for i, value in enumerate(unique):\n",
        "      lookup[value] = i\n",
        "   for row in dataset:\n",
        "      row[column] = lookup[row[column]]\n",
        "   return lookup"
      ],
      "metadata": {
        "id": "zKiNBDXpKYHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os valores de entrada variam em escala e precisam ser normalizados para o intervalo de 0 e 1. Geralmente, √© uma boa pr√°tica normalizar valores de entrada para o intervalo da fun√ß√£o de transfer√™ncia escolhida; nesse caso, a fun√ß√£o sigm√≥ide que gera valores entre 0 e 1 As fun√ß√µes auxiliar `dataset_minmax()` e `normalize_dataset()` foram usadas para normalizar os valores de entrada."
      ],
      "metadata": {
        "id": "I_mgXz1S4rJW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wX-KLBZUJ3Wh"
      },
      "outputs": [],
      "source": [
        "# Find the min and max values for each column\n",
        "def dataset_minmax(dataset):\n",
        "   minmax = list()\n",
        "   stats = [[min(column), max(column)] for column in zip(*dataset)]\n",
        "   return stats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rescale dataset columns to the range 0-1\n",
        "def normalize_dataset(dataset, minmax):\n",
        "   for row in dataset:\n",
        "      for i in range(len(row)-1):\n",
        "         row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])"
      ],
      "metadata": {
        "id": "cXt93tP9KfSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avaliaremos o algoritmo usando a valida√ß√£o cruzada com dobras k com 5 dobras. Isso significa que 201/5 = 40,2 ou 40 registros estar√£o em cada dobra. Usaremos as fun√ß√µes auxiliar `evaluate_algorithm` | `cross_validation_split` para avaliar o algoritmo com valida√ß√£o cruzada e `accuracy_metric` para calcular a precis√£o das previs√µes."
      ],
      "metadata": {
        "id": "m4GZ6VeD4uoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "   dataset_split = list()\n",
        "   dataset_copy = list(dataset)\n",
        "   fold_size = int(len(dataset) / n_folds)\n",
        "   for i in range(n_folds):\n",
        "      fold = list()\n",
        "      while len(fold) < fold_size:\n",
        "         index = randrange(len(dataset_copy))\n",
        "         fold.append(dataset_copy.pop(index))\n",
        "      dataset_split.append(fold)\n",
        "   return dataset_split"
      ],
      "metadata": {
        "id": "yB0YiTtsKhbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "   folds = cross_validation_split(dataset, n_folds)\n",
        "   scores = list()\n",
        "   for fold in folds:\n",
        "      train_set = list(folds)\n",
        "      train_set.remove(fold)\n",
        "      train_set = sum(train_set, [])\n",
        "      test_set = list()\n",
        "      for row in fold:\n",
        "         row_copy = list(row)\n",
        "         test_set.append(row_copy)\n",
        "         row_copy[-1] = None\n",
        "      predicted = algorithm(train_set, test_set, *args)\n",
        "      actual    = [row[-1] for row in fold]\n",
        "      accuracy  = accuracy_metric(actual, predicted)\n",
        "      scores.append(accuracy)\n",
        "   return scores"
      ],
      "metadata": {
        "id": "dPD64mnAKl5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "   correct = 0\n",
        "   for i in range(len(actual)):\n",
        "      if actual[i] == predicted[i]:\n",
        "         correct += 1\n",
        "   return correct / float(len(actual)) * 100.0"
      ],
      "metadata": {
        "id": "CPaWJUH0KjNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma nova fun√ß√£o chamada `back_propagation()` foi desenvolvida para gerenciar o aplicativo do algoritmo *Backpropagation*, primeiro inicializando uma rede, treinando-a no conjunto de dados de treinamento e depois usando a rede treinada para fazer previs√µes em um conjunto de dados de teste."
      ],
      "metadata": {
        "id": "whxbonSA5FrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
        "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
        "   n_inputs = len(train[0]) - 1\n",
        "   n_outputs = len(set([row[-1] for row in train]))\n",
        "   network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
        "   train_network(network, train, l_rate, n_epoch, n_outputs)\n",
        "   predictions = list()\n",
        "   for row in test:\n",
        "      prediction = predict(network, row)\n",
        "      predictions.append(prediction)\n",
        "   return(predictions)"
      ],
      "metadata": {
        "id": "org89katLEjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "O exemplo completo est√° listado abaixo. (as chamadas)"
      ],
      "metadata": {
        "id": "zWc4d3c25IFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Backprop on Seeds dataset\n",
        "seed(1)\n",
        "# load and prepare data\n",
        "filename = 'seeds_dataset.csv'\n",
        "dataset = load_csv(filename)\n",
        "\n",
        "for i in range(len(dataset[0])-1):\n",
        "   str_column_to_float(dataset, i)\n",
        "\n",
        "# convert class column to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "\n",
        "# normalize input variables\n",
        "minmax = dataset_minmax(dataset)\n",
        "normalize_dataset(dataset, minmax)\n",
        "\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "l_rate = 0.3\n",
        "n_epoch = 500\n",
        "n_hidden = 5\n",
        "\n",
        "scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90MAJ7RFKdli",
        "outputId": "63a001b1-d96e-4156-d375-fbfc67a9fe92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [95.23809523809523, 92.85714285714286, 97.61904761904762, 92.85714285714286, 90.47619047619048]\n",
            "Mean Accuracy: 93.810%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo Completo üå±"
      ],
      "metadata": {
        "id": "rQN1S2_q9b5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Backprop on the Seeds Dataset\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from random import random\n",
        "from csv import reader\n",
        "from math import exp\n",
        "\n",
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "   dataset = list()\n",
        "   with open(filename, 'r') as file:\n",
        "      csv_reader = reader(file)\n",
        "      for row in csv_reader:\n",
        "         if not row:\n",
        "            continue\n",
        "         dataset.append(row)\n",
        "   return dataset\n",
        "\n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "   for row in dataset:\n",
        "      row[column] = float(row[column].strip())\n",
        "\n",
        "# Convert string column to integer\n",
        "def str_column_to_int(dataset, column):\n",
        "   class_values = [row[column] for row in dataset]\n",
        "   unique = set(class_values)\n",
        "   lookup = dict()\n",
        "   for i, value in enumerate(unique):\n",
        "      lookup[value] = i\n",
        "   for row in dataset:\n",
        "      row[column] = lookup[row[column]]\n",
        "   return lookup\n",
        "\n",
        "# Find the min and max values for each column\n",
        "def dataset_minmax(dataset):\n",
        "   minmax = list()\n",
        "   stats = [[min(column), max(column)] for column in zip(*dataset)]\n",
        "   return stats\n",
        "\n",
        "# Rescale dataset columns to the range 0-1\n",
        "def normalize_dataset(dataset, minmax):\n",
        "   for row in dataset:\n",
        "      for i in range(len(row)-1):\n",
        "         row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
        "\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "   dataset_split = list()\n",
        "   dataset_copy = list(dataset)\n",
        "   fold_size = int(len(dataset) / n_folds)\n",
        "   for i in range(n_folds):\n",
        "      fold = list()\n",
        "      while len(fold) < fold_size:\n",
        "         index = randrange(len(dataset_copy))\n",
        "         fold.append(dataset_copy.pop(index))\n",
        "      dataset_split.append(fold)\n",
        "   return dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "   correct = 0\n",
        "   for i in range(len(actual)):\n",
        "      if actual[i] == predicted[i]:\n",
        "         correct += 1\n",
        "   return correct / float(len(actual)) * 100.0\n",
        "\n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "   folds = cross_validation_split(dataset, n_folds)\n",
        "   scores = list()\n",
        "   for fold in folds:\n",
        "      train_set = list(folds)\n",
        "      train_set.remove(fold)\n",
        "      train_set = sum(train_set, [])\n",
        "      test_set = list()\n",
        "      for row in fold:\n",
        "         row_copy = list(row)\n",
        "         test_set.append(row_copy)\n",
        "         row_copy[-1] = None\n",
        "      predicted = algorithm(train_set, test_set, *args)\n",
        "      actual    = [row[-1] for row in fold]\n",
        "      accuracy  = accuracy_metric(actual, predicted)\n",
        "      scores.append(accuracy)\n",
        "   return scores\n",
        "\n",
        "# Calculate neuron activation for an input\n",
        "def activate(weights, inputs):\n",
        "   activation = weights[-1]\n",
        "   for i in range(len(weights)-1):\n",
        "      activation += weights[i] * inputs[i]\n",
        "   return activation\n",
        "\n",
        "# Transfer neuron activation\n",
        "def transfer(activation):\n",
        "   return 1.0 / (1.0 + exp(-activation))\n",
        "\n",
        "# Forward propagate input to a network output\n",
        "def forward_propagate(network, row):\n",
        "   inputs = row\n",
        "   for layer in network:\n",
        "      new_inputs = []\n",
        "      for neuron in layer:\n",
        "         activation = activate(neuron['weights'], inputs)\n",
        "         neuron['output'] = transfer(activation)\n",
        "         new_inputs.append(neuron['output'])\n",
        "      inputs = new_inputs\n",
        "   return inputs\n",
        "\n",
        "# Calculate the derivative of an neuron output\n",
        "def transfer_derivative(output):\n",
        "   return output * (1.0 - output)\n",
        "\n",
        "# Backpropagate error and store in neurons\n",
        "def backward_propagate_error(network, expected):\n",
        "   for i in reversed(range(len(network))):\n",
        "      layer = network[i]\n",
        "      errors = list()\n",
        "      if i != len(network)-1:\n",
        "         for j in range(len(layer)):\n",
        "            error = 0.0\n",
        "            for neuron in network[i + 1]:\n",
        "               error += (neuron['weights'][j] * neuron['delta'])\n",
        "            errors.append(error)\n",
        "      else:\n",
        "         for j in range(len(layer)):\n",
        "            neuron = layer[j]\n",
        "            errors.append(expected[j] - neuron['output'])\n",
        "      for j in range(len(layer)):\n",
        "         neuron = layer[j]\n",
        "         neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
        "\n",
        "# Update network weights with error\n",
        "def update_weights(network, row, l_rate):\n",
        "   for i in range(len(network)):\n",
        "      inputs = row[:-1]\n",
        "      if i != 0:\n",
        "         inputs = [neuron['output'] for neuron in network[i - 1]]\n",
        "      for neuron in network[i]:\n",
        "         for j in range(len(inputs)):\n",
        "            neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
        "         neuron['weights'][-1] += l_rate * neuron['delta']\n",
        "\n",
        "# Train a network for a fixed number of epochs\n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "   for epoch in range(n_epoch):\n",
        "      for row in train:\n",
        "         outputs = forward_propagate(network, row)\n",
        "         expected = [0 for i in range(n_outputs)]\n",
        "         expected[row[-1]] = 1\n",
        "         backward_propagate_error(network, expected)\n",
        "         update_weights(network, row, l_rate)\n",
        "\n",
        "# Initialize a network\n",
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "   network = list()\n",
        "   hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
        "   network.append(hidden_layer)\n",
        "   output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
        "   network.append(output_layer)\n",
        "   return network\n",
        "\n",
        "# Make a prediction with a network\n",
        "def predict(network, row):\n",
        "   outputs = forward_propagate(network, row)\n",
        "   return outputs.index(max(outputs))\n",
        "\n",
        "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
        "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
        "   n_inputs = len(train[0]) - 1\n",
        "   n_outputs = len(set([row[-1] for row in train]))\n",
        "   network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
        "   train_network(network, train, l_rate, n_epoch, n_outputs)\n",
        "   predictions = list()\n",
        "   for row in test:\n",
        "      prediction = predict(network, row)\n",
        "      predictions.append(prediction)\n",
        "   return(predictions)\n",
        "\n",
        "# Test Backprop on Seeds dataset\n",
        "seed(1)\n",
        "\n",
        "# load and prepare data\n",
        "filename = 'seeds_dataset.csv'\n",
        "dataset = load_csv(filename)\n",
        "for i in range(len(dataset[0])-1):\n",
        "   str_column_to_float(dataset, i)\n",
        "\n",
        "# convert class column to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "\n",
        "# normalize input variables\n",
        "minmax = dataset_minmax(dataset)\n",
        "normalize_dataset(dataset, minmax)\n",
        "\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "l_rate = 0.3\n",
        "n_epoch = 500\n",
        "n_hidden = 5\n",
        "scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
        "\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRcExKVj7Umo",
        "outputId": "20c99929-00a4-4e6d-811c-2773cd346ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [95.23809523809523, 92.85714285714286, 97.61904761904762, 92.85714285714286, 90.47619047619048]\n",
            "Mean Accuracy: 93.810%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma rede com `5 neur√¥nios na camada oculta` e `3 neur√¥nios na camada de sa√≠da` foi constru√≠da. A rede foi treinada por `500 √©pocas` com uma taxa de aprendizado de `0,3`. Esses par√¢metros foram encontrados com um pouco de tentativa e erro, mas voc√™ pode fazer muito melhor.\n",
        "\n",
        "A execu√ß√£o do exemplo imprime a precis√£o m√©dia da classifica√ß√£o em cada dobra, bem como o desempenho m√©dio em todas as dobras.\n",
        "\n",
        "Voc√™ pode ver que a retropropaga√ß√£o e a configura√ß√£o escolhida alcan√ßaram uma precis√£o de classifica√ß√£o m√©dia de cerca de `93%`, o que √© muito melhor que o algoritmo de regra zero, que teve uma precis√£o ligeiramente melhor que `28%` de precis√£o."
      ],
      "metadata": {
        "id": "0S2zIfjK5f3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sugest√µes de Testes e melhorias"
      ],
      "metadata": {
        "id": "qacHLKak_hL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Ajustar par√¢metros do algoritmo**. Tente redes maiores ou menores treinadas por mais ou menos. Veja se voc√™ pode obter um melhor desempenho no conjunto de dados de sementes.\n",
        "- **M√©todos adicionais**. Experimente diferentes t√©cnicas de inicializa√ß√£o de peso (como pequenos n√∫meros aleat√≥rios) e diferentes fun√ß√µes de transfer√™ncia (como tanh).\n",
        "- **Mais camadas**. Adicione suporte para mais camadas ocultas, treinadas da mesma maneira que a camada oculta usada neste tutorial.\n",
        "- **Regress√£o**. Altere a rede para que haja apenas um neur√¥nio na camada de sa√≠da e que um valor real seja previsto. Escolha um conjunto de dados de regress√£o para praticar. Uma fun√ß√£o de transfer√™ncia linear pode ser usada para neur√¥nios na camada de sa√≠da ou os valores de sa√≠da do conjunto de dados escolhido podem ser redimensionados para valores entre 0 e 1.\n",
        "- **Descida em gradiente em lote (Batch Gradient Descent)**. Altere o procedimento de treinamento de online para descida em gradiente de lote e atualize os pesos somente no final de cada √©poca."
      ],
      "metadata": {
        "id": "Hu80cRms_kcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_2anD57Q_LoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mais informa√ß√µes sobre Machine Learning? üìö ü§ñ\n",
        "\n",
        "Blog do Zouza no Medium ([link](https://medium.com/blog-do-zouza/o-que-%C3%A9-machine-learning-5e7e98453985))"
      ],
      "metadata": {
        "id": "7OQNmXDa_IC7"
      }
    }
  ]
}